{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \"Blazingly fast and simple JSON Parser -- O( n ) Time and O( 1 ) Space\"\n",
    "\n",
    "> \"In this section we will delve into a very clever and efficient parsing algorithm for JSON data.The main idea of the parser is to scan through JSON string linearly and maintain a set of states which would indicate the beginning,end,type of various JSON objects without allocating any extra space\"\n",
    "\n",
    "JSON has been a time tested format for exchanges between independent nodes(may be a client and server node).A server may publish a protocol as to what keys and values it expects from the client to perform certain tasks. The client can send in request in JSON format and receive the result as well in a JSON format. This decouples two systems and they have just a understanding of the format they will be communicating in.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Note: The usage and the implementation of the parser is available [here](https://github.com/zserge/jsmn) . This blog intends to give a quick sense of how this parser works and see why it works so blazingly fast. We will also look at ways in which we can extend this parser for the case where we receive the JSON data bytes in streaming fashion and not all at once.It will be interesting to see how much of an impact that will have on the performance of the algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Tip: One of the main overheads for higly performant JSON request processing servers is parsing the JSON.Usually,parsers allocates a DOM object and then after parsing the JSON returns the entire tree for the server to query on the keys and get relevant values.But for servers where speed of processing the JSON requests is critical, even the allocating space for the DOM object every time a JSON request comes in to be serviced can bring down the performance of the server.The time spent in parsing the JSON requests is going to be always an overhead and hence it is good to have linear time and constant space parsers which in *normal* circumstances would not require any amount of memory allocation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example JSON string for running through the algorithm\n",
    "\n",
    "We will be running an example through our algorithm and we choose a JSON string for the same --"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "JSON_STRN=\"{'key1' : \"str_val1\",'key2':{'key21': num_val,'key22': primitive_val} ,'key3' : [\"str_val31\",\"str_val32\"]}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Important: JSON has only a limited number of types it supports. JSON supports object type,string type, array type and primitive types. Primitive types include numbers, boolean(true/false) and NULL values. To see how we can build a JSON object using the above types refer to this [resource](https://www.json.org/json-en.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data structure for parsing\n",
    "\n",
    "The goal of the parsing algorithm is to fill in an array of tokens as it scrolls through the JSON request. Each JSON component be it objects,arrays,strings,or primitive types is considered to be a token, and in this array we hold a few information about each of these tokens and *sub-tokens*. A picture might help to clearly see what this algorithm intends to do.\n",
    "\n",
    "![](my_icons/jsmn_parser.jpg)\n",
    "\n",
    "The highlighted segments in the JSON string is a token.A token in itself can contain sub-tokens(arrays and objects)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each token in the arrays of token holds the following information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "\n",
    "class JSON_type(Enum):\n",
    "    JSON_UNDEFINED = 0\n",
    "    JSON_OBJECT = 1\n",
    "    JSON_ARRAY = 2\n",
    "    JSON_STRING = 3\n",
    "    JSON_PRIMITIVE = 4\n",
    "class JSON_token:\n",
    "    def __init__(self,start: int = -1, stop: int = -1, size: int = -1, parent: int = -1,tok_type: JSON_type = 0):\n",
    "        self.start = start #the position in the JSON string where this token starts\n",
    "        self.stop = stop   #the position in the JSON string where this token ends\n",
    "        self.size = size   #the number of sub-tokens within this token\n",
    "        self.parent = parent #if this is a sub-token, what's the index of the its parent token\n",
    "        self.tok_type = tok_type #what the type of the token\n",
    "        \n",
    "#allocating an array of tokens\n",
    "max_possible_tokens = 128\n",
    "token_list = [JSON_token() for i in range(max_possible_tokens)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parsing Algorithm\n",
    "\n",
    "Here we have go to traverse down the JSON string and then based on each character update the list of token objects. The parser is robustly written to take care of all the corner cases issues etc. Here I just identify the cases and add comments as to what each case should handle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from typing import List\n",
    "def parser(tokens_list: List[JSON_token], json:str):\n",
    "    for idx,element in enumerate(json):\n",
    "        if element == '{' or ']':\n",
    "            None\n",
    "            #get the next availble token slot from tokens_list, if not available allocate more\n",
    "            #mark the token slot with appropriate type \n",
    "            #mark start as idx\n",
    "            #mark a parent variable to indicate this is going to be the parent token for the upcoming tokens\n",
    "        elif element == '}' or ']':\n",
    "            None\n",
    "            #go back in the tokens array and search for the parent token for this closure\n",
    "            #the above can be idenfied by start != -1 and end == -1\n",
    "            #fill the end the end value for this token marking end as idx\n",
    "            #reset the parent varibale appropriately\n",
    "        elif element == ':':\n",
    "            None\n",
    "            #here it is an idication a value is coming up next, so mark the previous token slot \n",
    "            #as the parent of thie upcoming onr\n",
    "        elif element == ',':\n",
    "            None\n",
    "            #here it is an indication of end of a key value pair and we will move to the next key:value pair\n",
    "            #update parent token field in precending string\n",
    "        elif element == '\\\"':\n",
    "            None\n",
    "            #here it's an indication of string so just traverse the string and fill in the values\n",
    "        else:\n",
    "            None\n",
    "            #here it's an indication of a primitve type so just traverse till the end of the type and fill in the values\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Warning: When we see one obvious limitation comes to mind - we have to know the number of tokens upfront for the JSON requests. This may not always be possible to guess. But again, there are a lot of services which limit the size of JSON request size and that size can be used to heuristically decide upon the size of the token arrays."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In any case to deal with the above possible limitation and also for request that come as a streaming request, i.e. not all the bytes are available to process the JSON request, we can modify the above algorithm to cover those cases as well, but we will have to pay with an increased time complexity for the same."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stream Parsing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Image a scenario where the client and the servers talk to each other via tcp protocol and the server has no control or knowledge of the buffer size at the client's end. This means that depending upon the difference in the sizes of the buffers, the full JSON request may not land up at the server's end. So we need a mechanism to be able to hold on to the relevant tokens that still may get its value filled at the arrival of a later batch of bytes. This will also deal with the issue of having to know the maximum number of tokens upfront. Again we can first imagine it visually and then have a look at how we can do it in code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:anaconda]",
   "language": "python",
   "name": "conda-env-anaconda-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

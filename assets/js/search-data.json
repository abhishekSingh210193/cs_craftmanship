{
  
    
        "post0": {
            "title": "Blazingly fast and simple JSON Parser -- O( n ) Time and O( 1 ) Space",
            "content": ". Note: The usage and the implementation of the parser is available here . This blog intends to give a quick sense of how this parser works and see why it works so blazingly fast. We will also look at ways in which we can extend this parser for the case where we receive the JSON data bytes in streaming fashion and not all at once.It will be interesting to see how much of an impact that will have on the performance of the algorithm. . . Tip: One of the main overheads for higly performant JSON request processing servers is parsing the JSON.Usually,parsers allocates a DOM object and then after parsing the JSON returns the entire tree for the server to query on the keys and get relevant values.But for servers where speed of processing the JSON requests is critical, even the allocating space for the DOM object every time a JSON request comes in to be serviced can bring down the performance of the server.The time spent in parsing the JSON requests is going to be always an overhead and hence it is good to have linear time and constant space parsers which in normal circumstances would not require any amount of memory allocation. . Example JSON string for running through the algorithm . We will be running an example through our algorithm and we choose a JSON string for the same -- . JSON_STRN=&quot;{&#39;key1&#39; : &quot;str_val1&quot;,&#39;key2&#39;:{&#39;key21&#39;: num_val,&#39;key22&#39;: primitive_val} ,&#39;key3&#39; : [&quot;str_val31&quot;,&quot;str_val32&quot;]}&quot; . . Important: JSON has only a limited number of types it supports. JSON supports object type,string type, array type and primitive types. Primitive types include numbers, boolean(true/false) and NULL values. To see how we can build a JSON object using the above types refer to this resource . Data structure for parsing . The goal of the parsing algorithm is to fill in an array of tokens as it scrolls through the JSON request. Each JSON component be it objects,arrays,strings,or primitive types is considered to be a token, and in this array we hold a few information about each of these tokens and sub-tokens. A picture might help to clearly see what this algorithm intends to do. . . The highlighted segments in the JSON string is a token.A token in itself can contain sub-tokens(arrays and objects). . Each token in the arrays of token holds the following information . from enum import Enum class JSON_type(Enum): JSON_UNDEFINED = 0 JSON_OBJECT = 1 JSON_ARRAY = 2 JSON_STRING = 3 JSON_PRIMITIVE = 4 class JSON_token: def __init__(self,start: int = -1, stop: int = -1, size: int = -1, parent: int = -1,tok_type: JSON_type = 0): self.start = start #the position in the JSON string where this token starts self.stop = stop #the position in the JSON string where this token ends self.size = size #the number of sub-tokens within this token self.parent = parent #if this is a sub-token, what&#39;s the index of the its parent token self.tok_type = tok_type #what the type of the token #allocating an array of tokens max_possible_tokens = 128 token_list = [JSON_token() for i in range(max_possible_tokens)] . Parsing Algorithm . Here we have go to traverse down the JSON string and then based on each character update the list of token objects. The parser is robustly written to take care of all the corner cases issues etc. Here I just identify the cases and add comments as to what each case should handle. . from typing import List def parser(tokens_list: List[JSON_token], json:str): for idx,element in enumerate(json): if element == &#39;{&#39; or &#39;]&#39;: None #get the next availble token slot from tokens_list, if not available allocate more #mark the token slot with appropriate type #mark start as idx #mark a parent variable to indicate this is going to be the parent token for the upcoming tokens elif element == &#39;}&#39; or &#39;]&#39;: None #go back in the tokens array and search for the parent token for this closure #the above can be idenfied by start != -1 and end == -1 #fill the end the end value for this token marking end as idx #reset the parent varibale appropriately elif element == &#39;:&#39;: None #here it is an idication a value is coming up next, so mark the previous token slot #as the parent of thie upcoming onr elif element == &#39;,&#39;: None #here it is an indication of end of a key value pair and we will move to the next key:value pair #update parent token field in precending string elif element == &#39; &quot;&#39;: None #here it&#39;s an indication of string so just traverse the string and fill in the values else: None #here it&#39;s an indication of a primitve type so just traverse till the end of the type and fill in the values . . Warning: When we see one obvious limitation comes to mind - we have to know the number of tokens upfront for the JSON requests. This may not always be possible to guess. But again, there are a lot of services which limit the size of JSON request size and that size can be used to heuristically decide upon the size of the token arrays. . In any case to deal with the above possible limitation and also for request that come as a streaming request, i.e. not all the bytes are available to process the JSON request, we can modify the above algorithm to cover those cases as well, but we will have to pay with an increased time complexity for the same. . Stream Parsing . Image a scenario where the client and the servers talk to each other via tcp protocol and the server has no control or knowledge of the buffer size at the client&#39;s end. This means that depending upon the difference in the sizes of the buffers, the full JSON request may not land up at the server&#39;s end. So we need a mechanism to be able to hold on to the relevant tokens that still may get its value filled at the arrival of a later batch of bytes. This will also deal with the issue of having to know the maximum number of tokens upfront. Again we can first imagine it visually and then have a look at how we can do it in code. .",
            "url": "https://abhisheksingh210193.github.io/cs_craftmanship/2021/01/04/Super-fast-JSON-parser.html",
            "relUrl": "/2021/01/04/Super-fast-JSON-parser.html",
            "date": " • Jan 4, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "Data structures for fast infinte batching or streaming requests processing",
            "content": ". Note: In this blog we will discuss a &quot;lock-free&quot; circular queue data structure called disruptor. It was designed to be an efficient concurrent message passing datastructure.The official implementations and other discussions are available here. This blog intends to summarise its use case and show the points where the design of the disruptor scores big. . LOCKS ARE BAD . Whenever we have a scenario where mutliple concurrent running threads contend on a shared data structure and you need to ensure visibility of changes (i.e. a consumer thread can only get its hand over the data after the producer has processed it and put it for further processing). The usual and most common way to ensure these two requirements is to use a lock. Locks need the operating system to arbitrate which thread has the responsibility on a shared piece of data. The operating system might schedule other processes and the software&#39;s thread may be waiting in a queue. Moreover, if other threads get scheduled by the CPU then the cache memory of the softwares&#39;s thread will be overwritten and when it finally gets access to the CPU, it may have to go as far as the main memory to get it&#39;s required data. All this adds a lot of overhead and is evident by the simple experiment of incrementing a single shared variable. In the experiment below we increment a shared variable in three different ways. In the first case, we have a single process incrementing the variable, in the second case we again have two threads, but they synchronize their way through the operation using locks. In the third case, we have two threads which increment the variables and they synchronize their operation using atomic locks. . SINGLE PROCESS INCREMENTING A SINGLE VARIABLE . import time def single_thread(): start = time.time() x = 0 for i in range(500000000): x += 1 end = time.time() return(end-start) print(single_thread()) . 28.66362190246582 . class SingleThreadedCounter(): def __init__(self): self.val = 0 def increment(self): self.val += 1 . TWO PROCESS INCREMENTING A SINGLE VARIABLE . import time from threading import Thread, Lock mutex = Lock() x = 0 def thread_fcn(): global x mutex.acquire() for i in range(250000000): x += 1 mutex.release() def mutex_increment(): start = time.time() t1 = Thread(target=thread_fcn) t2 = Thread(target=thread_fcn) t1.start() t2.start() t1.join() t2.join() end = time.time() return (end-start) print(mutex_increment()) . 36.418396949768066 . . Note: As we can see that the time for performing the increment operation has gone up substantially when we would have expected it take half the time. . . Important: In the rest of the blog we will take in a very usual scenario we see in streaming request processing. A client sends in requests to a server in a streaming fashion. The server at its end needs to process the client&#39;s request, it may have multiple stages of processing. For example, imagine the client sends in a stream of requests and the server in JSON format. Now the probable first task that the client needs to perform is to parse the JSON request.Imagine a thread being assigned to do this parsing task. It parses requests one after another and hands over the parsed request in some form to another thread which may be responsible for performing business logic for that client. Usually the data structure to manage this message passing and flow control in screaming scenario is handled by a queue data structure. The producer threads (parser thread) puts in parsed data in this queue, from which the consumer thread (the business logic thread) will read of the parsed data. Because we have two threads working concurrently on a single data structure (the queue) we can expect contention to kick in. . WHY QUEUES ARE FLAWED . The queue could be an obvious choice for handling data communication between multiple threads, but the queue data structure is fundamentally flawed for communication between multiple threads. Imagine the case of the first two threads of the a system using a queue for data communication, the listener thread and the parsing thread. The listener thread listens to bytes from the wire and puts it in a queue and the parser thread will pick up bytes from the queue and parse it. Typically, a queue data structure will have a head field, a tail field and a size field (to tell an empty queue from a full one). The head field will be modified by the parser thread and the tail field by the parser thread. The size field though will be modified by both of the threads and it effectively makes the queue data structure having two writers. . Moreover, the entire data structure will fall in the same cache line and hence when say the listener thread modifies the tail field, the head field in another core also gets invalidated and needs to be fetched from a level 2 cache. . CAN WE AVOID LOCKS ? . So, using a queue structure for inter-thread communication with expensive locks could cost a lot of performance for any system. Hence, we move towards a better data structure that solves the issues of synchronization among threads. The data structure we use doesn&#39;t use locks. The main components of the data structure are - A. A circular buffer B. A sequence number field which has a number indicating a specific slot in the circular buffer. C. Each of the worker threads have their own sequence number. The circular buffer is written to by the producers . The producer in each case updates the sequence number for each of the circular buffers. The worker threads (consumer thread) have their own sequence number indicating the slots they have consumed so far from the circular buffer. . . Note: In the design, each of the elements has a SINGLE WRITER. The producer threads of the circular ring write to the ring buffer, and its sequence number. The worker consumer threads will write their own local sequence number. No field or data have more than one writer in this data structure. . WRITE OPERATION ON THE LOCK-FREE DATA STRUCTURE . Before writing a slot in the circular buffer, the thread has to make sure that it doesn&#39;t overwrite old bytes that have not yet been processed by the consumer thread. The consumer thread also maintains a sequence number, this number indicates the slots that have been already processed. So the producer thread before writing grabs the circular buffer&#39;s sequence number, adds one to it (mod size of the circular buffer) to get the next eligible slot for writing. But before putting in the bytes in that slot it checks with the dependent consumer thread (by reading their local sequence number) if they have processed this slot. If say the consumer has not yet processed this slot, then the producer thread goes in a busy wait till the slot is available to write to. When the slot is overwritten then the circular buffer&#39;s sequence number is updated by the producer thread. This indicates to consumer threads that they have a new slot to consume. . Writing to the circular buffers is a 2-phase commit. In the first phase, we check out a slot from the circular buffer. We can only check out a slot if it has already been consumed. This is ensured by following the logic mentioned above. Once the slot is checked out the producer writes the next byte to it. Then it sends a commit message to commit the entry by updating the circular buffer&#39;s sequence number to its next logical value(+1 mod size of the circular buffer) . READ OPERATION ON THE LOCK_FREE DATA STRUCTURE . The consumer thread reads the slots from circular buffer -1. Before reading the next slot, it checks (read) the buffer&#39;s sequence number. This number is indicative of the slots till which the buffer can read. . ENSURING THAT THE READS HAPPEN IN PROGRAM ORDER . There is just one piece of detail that needs to be addressed for the above data structure to work. Compilers and CPU take the liberty to reorder independent instructions for optimizations. This doesn’t have any issues in the single process case where the program’s logic integrity is maintained. But this could logic breakdown in case of multiple threads. Imagine a typical simplified read/write to the circular buffer described above— Say the publisher thread’s sequence of operation is indicated in black, and the consumer thread’s in brown. The publisher checks in a slot and it updates the sequence number. Then the consumer thread reads the (wrong) sequence number of the buffer and goes on to access the slot which is yet to be written. . The way we could solve this is by putting memory fences around the variables which tells the compiler and CPU to not reorder reads / writes before and after those shared variables. In that way programs logic integrity is maintained. .",
            "url": "https://abhisheksingh210193.github.io/cs_craftmanship/2021/01/04/Lock-free-data-structures.html",
            "relUrl": "/2021/01/04/Lock-free-data-structures.html",
            "date": " • Jan 4, 2021"
        }
        
    
  
    
        ,"post2": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master - badges: true - comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . place a #collapse-output flag at the top of any cell if you want to put the output under a collapsable element that is closed by default, but give the reader the option to open it: . print(&#39;The comment #collapse-output was used to collapse the output of this cell by default but you can expand it.&#39;) . The comment #collapse-output was used to collapse the output of this cell by default but you can expand it. . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(df).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(df).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( alt.X(&#39;Rotten_Tomatoes_Rating&#39;, type=&#39;quantitative&#39;), alt.Y(&#39;IMDB_Rating&#39;, type=&#39;quantitative&#39;, axis=alt.Axis(minExtent=30)), # y=alt.Y(&#39;IMDB_Rating:Q&#39;, ), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=500, height=400 ) . Example 3: More Tooltips . label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=500, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://abhisheksingh210193.github.io/cs_craftmanship/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " • Feb 20, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://abhisheksingh210193.github.io/cs_craftmanship/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://abhisheksingh210193.github.io/cs_craftmanship/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://abhisheksingh210193.github.io/cs_craftmanship/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}
{
  
    
        "post0": {
            "title": "Blazingly fast and simple JSON Parser -- O( n ) Time and O( 1 ) Space",
            "content": ". Note: The usage and the implementation of the parser is available here . This blog intends to give a quick sense of how this parser works and see why it works so blazingly fast. We will also look at ways in which we can extend this parser for the case where we receive the JSON data bytes in streaming fashion and not all at once.It will be interesting to see how much of an impact that will have on the performance of the algorithm. . . Tip: One of the main overheads for higly performant JSON request processing servers is parsing the JSON.Usually,parsers allocates a DOM object and then after parsing the JSON returns the entire tree for the server to query on the keys and get relevant values.But for servers where speed of processing the JSON requests is critical, even the allocating space for the DOM object every time a JSON request comes in to be serviced can bring down the performance of the server.The time spent in parsing the JSON requests is going to be always an overhead and hence it is good to have linear time and constant space parsers which in normal circumstances would not require any amount of memory allocation. . Example JSON string for running through the algorithm . We will be running an example through our algorithm and we choose a JSON string for the same -- . JSON_STRN=&quot;{&#39;key1&#39; : &quot;str_val1&quot;,&#39;key2&#39;:{&#39;key21&#39;: num_val,&#39;key22&#39;: primitive_val} ,&#39;key3&#39; : [&quot;str_val31&quot;,&quot;str_val32&quot;]}&quot; . . Important: JSON has only a limited number of types it supports. JSON supports object type,string type, array type and primitive types. Primitive types include numbers, boolean(true/false) and NULL values. To see how we can build a JSON object using the above types refer to this resource . Data structure for parsing . The goal of the parsing algorithm is to fill in an array of tokens as it scrolls through the JSON request. Each JSON component be it objects,arrays,strings,or primitive types is considered to be a token, and in this array we hold a few information about each of these tokens and sub-tokens. A picture might help to clearly see what this algorithm intends to do. . . The highlighted segments in the JSON string is a token.A token in itself can contain sub-tokens(arrays and objects). . Each token in the arrays of token holds the following information . from enum import Enum class JSON_type(Enum): JSON_UNDEFINED = 0 JSON_OBJECT = 1 JSON_ARRAY = 2 JSON_STRING = 3 JSON_PRIMITIVE = 4 class JSON_token: def __init__(self,start: int = -1, stop: int = -1, size: int = -1, parent: int = -1,tok_type: JSON_type = 0): self.start = start #the position in the JSON string where this token starts self.stop = stop #the position in the JSON string where this token ends self.size = size #the number of sub-tokens within this token self.parent = parent #if this is a sub-token, what&#39;s the index of the its parent token self.tok_type = tok_type #what the type of the token #allocating an array of tokens max_possible_tokens = 128 token_list = [JSON_token() for i in range(max_possible_tokens)] . Parsing Algorithm . Here we have go to traverse down the JSON string and then based on each character update the list of token objects. The parser is robustly written to take care of all the corner cases issues etc. Here I just identify the cases and add comments as to what each case should handle. . from typing import List def parser(tokens_list: List[JSON_token], json:str): for idx,element in enumerate(json): if element == &#39;{&#39; or &#39;]&#39;: None #get the next availble token slot from tokens_list, if not available allocate more #mark the token slot with appropriate type #mark start as idx #mark a parent variable to indicate this is going to be the parent token for the upcoming tokens elif element == &#39;}&#39; or &#39;]&#39;: None #go back in the tokens array and search for the parent token for this closure #the above can be idenfied by start != -1 and end == -1 #fill the end the end value for this token marking end as idx #reset the parent varibale appropriately elif element == &#39;:&#39;: None #here it is an idication a value is coming up next, so mark the previous token slot #as the parent of thie upcoming onr elif element == &#39;,&#39;: None #here it is an indication of end of a key value pair and we will move to the next key:value pair #update parent token field in precending string elif element == &#39; &quot;&#39;: None #here it&#39;s an indication of string so just traverse the string and fill in the values else: None #here it&#39;s an indication of a primitve type so just traverse till the end of the type and fill in the values . . Warning: When we see one obvious limitation comes to mind - we have to know the number of tokens upfront for the JSON requests. This may not always be possible to guess. But again, there are a lot of services which limit the size of JSON request size and that size can be used to heuristically decide upon the size of the token arrays. . In any case to deal with the above possible limitation and also for request that come as a streaming request, i.e. not all the bytes are available to process the JSON request, we can modify the above algorithm to cover those cases as well, but we will have to pay with an increased time complexity for the same. . Stream Parsing . Image a scenario where the client and the servers talk to each other via tcp protocol and the server has no control or knowledge of the buffer size at the client&#39;s end. This means that depending upon the difference in the sizes of the buffers, the full JSON request may not land up at the server&#39;s end. So we need a mechanism to be able to hold on to the relevant tokens that still may get its value filled at the arrival of a later batch of bytes. This will also deal with the issue of having to know the maximum number of tokens upfront. Again we can first imagine it visually and then have a look at how we can do it in code. .",
            "url": "https://abhisheksingh210193.github.io/cs_craftsmanship/2021/01/04/Super-fast-JSON-parser.html",
            "relUrl": "/2021/01/04/Super-fast-JSON-parser.html",
            "date": " • Jan 4, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "Data structures for fast infinte batching or streaming requests processing",
            "content": ". Note: In this blog we will discuss a &quot;lock-free&quot; circular queue data structure called disruptor. It was designed to be an efficient concurrent message passing datastructure.The official implementations and other discussions are available here. This blog intends to summarise its use case and show the points where the design of the disruptor scores big. . LOCKS ARE BAD . Whenever we have a scenario where mutliple concurrent running threads contend on a shared data structure and you need to ensure visibility of changes (i.e. a consumer thread can only get its hand over the data after the producer has processed it and put it for further processing). The usual and most common way to ensure these two requirements is to use a lock. Locks need the operating system to arbitrate which thread has the responsibility on a shared piece of data. The operating system might schedule other processes and the software&#39;s thread may be waiting in a queue. Moreover, if other threads get scheduled by the CPU then the cache memory of the softwares&#39;s thread will be overwritten and when it finally gets access to the CPU, it may have to go as far as the main memory to get it&#39;s required data. All this adds a lot of overhead and is evident by the simple experiment of incrementing a single shared variable. In the experiment below we increment a shared variable in three different ways. In the first case, we have a single process incrementing the variable, in the second case we again have two threads, but they synchronize their way through the operation using locks. In the third case, we have two threads which increment the variables and they synchronize their operation using atomic locks. . SINGLE PROCESS INCREMENTING A SINGLE VARIABLE . import time def single_thread(): start = time.time() x = 0 for i in range(500000000): x += 1 end = time.time() return(end-start) print(single_thread()) . 28.66362190246582 . class SingleThreadedCounter(): def __init__(self): self.val = 0 def increment(self): self.val += 1 . TWO PROCESS INCREMENTING A SINGLE VARIABLE . import time from threading import Thread, Lock mutex = Lock() x = 0 def thread_fcn(): global x mutex.acquire() for i in range(250000000): x += 1 mutex.release() def mutex_increment(): start = time.time() t1 = Thread(target=thread_fcn) t2 = Thread(target=thread_fcn) t1.start() t2.start() t1.join() t2.join() end = time.time() return (end-start) print(mutex_increment()) . 36.418396949768066 . . Note: As we can see that the time for performing the increment operation has gone up substantially when we would have expected it take half the time. . . Important: In the rest of the blog we will take in a very usual scenario we see in streaming request processing. A client sends in requests to a server in a streaming fashion. The server at its end needs to process the client&#39;s request, it may have multiple stages of processing. For example, imagine the client sends in a stream of requests and the server in JSON format. Now the probable first task that the client needs to perform is to parse the JSON request.Imagine a thread being assigned to do this parsing task. It parses requests one after another and hands over the parsed request in some form to another thread which may be responsible for performing business logic for that client. Usually the data structure to manage this message passing and flow control in screaming scenario is handled by a queue data structure. The producer threads (parser thread) puts in parsed data in this queue, from which the consumer thread (the business logic thread) will read of the parsed data. Because we have two threads working concurrently on a single data structure (the queue) we can expect contention to kick in. . WHY QUEUES ARE FLAWED . The queue could be an obvious choice for handling data communication between multiple threads, but the queue data structure is fundamentally flawed for communication between multiple threads. Imagine the case of the first two threads of the a system using a queue for data communication, the listener thread and the parsing thread. The listener thread listens to bytes from the wire and puts it in a queue and the parser thread will pick up bytes from the queue and parse it. Typically, a queue data structure will have a head field, a tail field and a size field (to tell an empty queue from a full one). The head field will be modified by the parser thread and the tail field by the parser thread. The size field though will be modified by both of the threads and it effectively makes the queue data structure having two writers. . Moreover, the entire data structure will fall in the same cache line and hence when say the listener thread modifies the tail field, the head field in another core also gets invalidated and needs to be fetched from a level 2 cache. . CAN WE AVOID LOCKS ? . So, using a queue structure for inter-thread communication with expensive locks could cost a lot of performance for any system. Hence, we move towards a better data structure that solves the issues of synchronization among threads. The data structure we use doesn&#39;t use locks. The main components of the data structure are - A. A circular buffer B. A sequence number field which has a number indicating a specific slot in the circular buffer. C. Each of the worker threads have their own sequence number. The circular buffer is written to by the producers . The producer in each case updates the sequence number for each of the circular buffers. The worker threads (consumer thread) have their own sequence number indicating the slots they have consumed so far from the circular buffer. . . Note: In the design, each of the elements has a SINGLE WRITER. The producer threads of the circular ring write to the ring buffer, and its sequence number. The worker consumer threads will write their own local sequence number. No field or data have more than one writer in this data structure. . WRITE OPERATION ON THE LOCK-FREE DATA STRUCTURE . Before writing a slot in the circular buffer, the thread has to make sure that it doesn&#39;t overwrite old bytes that have not yet been processed by the consumer thread. The consumer thread also maintains a sequence number, this number indicates the slots that have been already processed. So the producer thread before writing grabs the circular buffer&#39;s sequence number, adds one to it (mod size of the circular buffer) to get the next eligible slot for writing. But before putting in the bytes in that slot it checks with the dependent consumer thread (by reading their local sequence number) if they have processed this slot. If say the consumer has not yet processed this slot, then the producer thread goes in a busy wait till the slot is available to write to. When the slot is overwritten then the circular buffer&#39;s sequence number is updated by the producer thread. This indicates to consumer threads that they have a new slot to consume. . Writing to the circular buffers is a 2-phase commit. In the first phase, we check out a slot from the circular buffer. We can only check out a slot if it has already been consumed. This is ensured by following the logic mentioned above. Once the slot is checked out the producer writes the next byte to it. Then it sends a commit message to commit the entry by updating the circular buffer&#39;s sequence number to its next logical value(+1 mod size of the circular buffer) . READ OPERATION ON THE LOCK_FREE DATA STRUCTURE . The consumer thread reads the slots from circular buffer -1. Before reading the next slot, it checks (read) the buffer&#39;s sequence number. This number is indicative of the slots till which the buffer can read. . ENSURING THAT THE READS HAPPEN IN PROGRAM ORDER . There is just one piece of detail that needs to be addressed for the above data structure to work. Compilers and CPU take the liberty to reorder independent instructions for optimizations. This doesn’t have any issues in the single process case where the program’s logic integrity is maintained. But this could logic breakdown in case of multiple threads. Imagine a typical simplified read/write to the circular buffer described above— Say the publisher thread’s sequence of operation is indicated in black, and the consumer thread’s in brown. The publisher checks in a slot and it updates the sequence number. Then the consumer thread reads the (wrong) sequence number of the buffer and goes on to access the slot which is yet to be written. . The way we could solve this is by putting memory fences around the variables which tells the compiler and CPU to not reorder reads / writes before and after those shared variables. In that way programs logic integrity is maintained. .",
            "url": "https://abhisheksingh210193.github.io/cs_craftsmanship/2021/01/04/Lock-free-data-structures.html",
            "relUrl": "/2021/01/04/Lock-free-data-structures.html",
            "date": " • Jan 4, 2021"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://abhisheksingh210193.github.io/cs_craftsmanship/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://abhisheksingh210193.github.io/cs_craftsmanship/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}